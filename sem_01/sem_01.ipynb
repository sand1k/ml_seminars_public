{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "matched-baseline",
   "metadata": {},
   "source": [
    "# Подготовка окружения\n",
    "\n",
    "## Python Installation\n",
    "\n",
    "We highly recommend using anaconda for installing python. [Click here](https://www.anaconda.com/download/) to go to Anaconda's download page. [Click here](https://docs.conda.io/en/latest/miniconda.html) to go on Miniconda's download page. Make sure to download Python 3.6 version.\n",
    "If you are on a windows machine:\n",
    " - Open the executable after download is complete and follow instructions.\n",
    " - Once installation is complete, open `Anaconda prompt` from the start menu. This will open a terminal with python enabled.\n",
    " \n",
    " If you are on a linux machine: \n",
    " \n",
    " - Open a terminal and navigate to the directory where Anaconda was downloaded. \n",
    " - Change the permission to the downloaded file so that it can be executed. So if the downloaded file name is `Anaconda3-5.1.0-Linux-x86_64.sh`, then use the following command:\n",
    " \n",
    "      `chmod a+x Anaconda3-5.1.0-Linux-x86_64.sh`\n",
    " \n",
    " - Now, run the installation script using `./Anaconda3-5.1.0-Linux-x86_64.sh`, and follow installation instructions in the terminal.\n",
    " \n",
    " \n",
    "Once you have installed python, create a new python environment will all the requirements using the following command: \n",
    "\n",
    "    conda create -n machine_learning python scipy numpy matplotlib jupyter\n",
    " \n",
    "After the new environment is setup, activate it using (windows)\n",
    "\n",
    "    activate machine_learning\n",
    "   \n",
    "or if you are on a linux machine\n",
    "\n",
    "    source activate machine_learning \n",
    "\n",
    "Now we have our python environment all set up, we can start working on the assignments. To do so, navigate to the directory where the assignments were installed, and launch the jupyter notebook from the terminal using the command\n",
    "\n",
    "    jupyter notebook\n",
    "\n",
    "## Python Tutorials\n",
    "\n",
    "If you are new to python and to `jupyter` notebooks, no worries! There is a plethora of tutorials and documentation to get you started. Here are a few links which might be of help:\n",
    "\n",
    "- [Python Programming](https://pythonprogramming.net/introduction-to-python-programming/): A tutorial with videos about the basics of python. \n",
    "\n",
    "- [Numpy and matplotlib tutorial](http://cs231n.github.io/python-numpy-tutorial/): We will be using numpy extensively for matrix and vector operations. This is great tutorial to get you started with using numpy and matplotlib for plotting.\n",
    "\n",
    "- [Jupyter notebook](https://medium.com/codingthesmartway-com-blog/getting-started-with-jupyter-notebook-for-python-4e7082bd5d46): Getting started with the jupyter notebook. \n",
    "\n",
    "- [Python introduction based on the class's MATLAB tutorial](https://github.com/mstampfer/Coursera-Stanford-ML-Python/blob/master/Coursera%20Stanford%20ML%20Python%20wiki.ipynb): This is the equivalent of class's MATLAB tutorial, in python.\n",
    "\n",
    "- [Numpy documentation](https://numpy.org/doc/stable/contents.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-demographic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.set_printoptions(linewidth=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greek-expansion",
   "metadata": {},
   "source": [
    "# Пример задачи ML: Ирисы Фишера\n",
    "Выборка взята отсюда: https://archive.ics.uci.edu/ml/datasets/iris\n",
    "## Загрузка выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data/iris.csv', \n",
    "                      header=None, \n",
    "                      names=['длина чашелистика', 'ширина чашелистика', \n",
    "                             'длина лепестка', 'ширина лепестка', 'класс'])\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56541c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-shock",
   "metadata": {},
   "source": [
    "## Начало работы с данными\n",
    "1. Определить множество объектов:\n",
    "    * Определить размер выборки\n",
    "    * Определить признаки, которыми описываются объекты\n",
    "2. Определить множество ответов\n",
    "3. Определить тип задачи машинного обучения\n",
    "6. ...\n",
    "\n",
    "### Множество объектов\n",
    "В данной задачи множество объектов описывается $n=4$ признаками:\n",
    "1. Длина чашелистика\n",
    "2. Ширина чашелистика\n",
    "3. Длина лепестка\n",
    "4. Ширина лепестка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Размер выборки составляет l={} объектов.'.format(dataset.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e223e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-mattress",
   "metadata": {},
   "source": [
    "Все признаки являются вещественными признаками. Формально объекты $\\mathbf{X}$ представляються в следующем виде:\n",
    "$$\\mathbf{X} \\in \\mathbb{R}^{l\\times n},$$\n",
    "где $l$ число объектов, а $n$ число признаков.\n",
    "\n",
    "Получаем, что $\\mathbf{X}$ это некоторая вещественная матрица размера $l\\times n$.\n",
    "\n",
    "### Множество ответов\n",
    "В данной задаче множество ответов состоит из трех элементов:\n",
    "1. Iris-virginica\n",
    "2. Iris-versicolor\n",
    "3. Iris-setosa\n",
    "\n",
    "### Задача машинного обучения\n",
    "В нашем случае, так как мощность множества $|\\mathbf{y}|=3 \\ll l=150$ получаем задачу классификации на $M=3$ класса.\n",
    "\n",
    "## Анализ данных\n",
    "Сначала проэктируем все объекты на двумерные плоскости, для упрощения анализа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset, hue='класс', height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-briefing",
   "metadata": {},
   "source": [
    "## Построение модели\n",
    "### Преобразование данных\n",
    "Как было сказано ранее нам требуется решить задачу классификации на 3 класса. Но для наглядноси рассмотрим бинарную классификацию (классификацию на несколько классов рассмотрим в одной из следующих лекций).\n",
    "\n",
    "Чтобы исходную задачу преобразовать в задачу бинарной классификации уберем из выборки все объекты класса Iris-setosa.\n",
    "Классы закодируем целыми числами  −1  и  1. Получим задачу бинарной классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-tongue",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset = dataset.drop(index = dataset.index[dataset['класс'] == 'Iris-setosa'])\n",
    "binary_dataset.loc[dataset['класс'] == 'Iris-versicolor', 'класс'] = -1\n",
    "binary_dataset.loc[dataset['класс'] == 'Iris-virginica', 'класс'] = 1\n",
    "binary_dataset.reset_index(inplace=True, drop=True)\n",
    "binary_dataset.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a52bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dataset.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "promotional-bunch",
   "metadata": {},
   "source": [
    "### Модель алгоритмов\n",
    "\n",
    "Модель алгоритмов $A$ в машинном обучении это некоторое множество функций, которые действуют из множества объектов в множество ответов, в нашем случае:\n",
    "$$A = \\{h| h: \\mathbb{R}^n \\to \\{-1, 1\\},$$\n",
    "обычно $A$ это некоторое параметрическое семество функций, тоесть разные функции $h$ отличаются друг от друга только каким-то параметром. Простым примером параметрического семейства функций для задачи бинарной классификации является семейство линейных классификаторов:\n",
    "$$A_{bcl} = \\left\\{h\\bigr(\\theta, \\mathbf{x}\\bigr)=\\text{sign}\\bigr(\\theta^{\\mathsf{T}}\\mathbf{x}\\bigr)\\bigr| \\theta \\in \\mathbb{R}^{n} \\right\\}.$$\n",
    "\n",
    "### Функция потерь\n",
    "\n",
    "Машиное обучение это всегда выбор функции из множества $A$. Чтобы выбрать функцию, нужен некоторый критерий по которому она выбирается, то есть нужно упоррядочить все функции от худшей к лучшей. Для этого построем функционал $Q$, который каждой функции $h \\in A$ ставит в соответствии число из $\\mathbb{R}_+$. В машинном обучении обычно функционал качества водиться как некоторая ошибка на выборке. В общем виде функционал качества можно представить в следующем виде:\n",
    "$$Q\\bigr(h, \\mathbf{X}, \\mathbf{y}\\bigr) = \\sum_{i=1}^l\\mathcal{L}\\bigr(h, \\mathbf{x}_i, y_i\\bigr),$$\n",
    "где $\\mathcal{L}$ некоторая функция ошибки (функция потерь) на некотором объекте $\\mathbf{x}$. Функционал качества $Q$ называется эмпирическим риском.\n",
    "\n",
    "### Оптимизационная задача\n",
    "\n",
    "Далее нужно поставить задачу оптимизации для выбора $h \\in A$. Здесь все просто, просто минимизируем эмпирический риск:\n",
    "$$\\hat{h} = \\arg \\min_{h \\in A} Q\\bigr(h, \\mathbf{X}, \\mathbf{y}\\bigr).$$\n",
    "\n",
    "Важно! В результе функция $\\hat{h}$ зависит от выборки $\\left(\\mathbf{X}, \\mathbf{y}\\right)$, то есть для разных наборов данных оптимальная функция будет различная.\n",
    "\n",
    "Вернемся к нашей задаче. В нашем случае функционал качества будет иметь следующий вид:\n",
    "$$Q\\bigr(\\theta, \\mathbf{X}, \\mathbf{y}\\bigr) = \\sum_{i=1}^l\\bigr[h\\bigr(\\theta, \\mathbf{x}_i\\bigr) \\not= y_i\\bigr],$$\n",
    "и оптимизационная задача переписывается в виде:\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta \\in \\mathbb{R}^n} \\sum_{i=1}^l\\bigr[h\\bigr(\\theta, \\mathbf{x}_i\\bigr) \\not= y_i\\bigr].$$\n",
    "\n",
    "И на самом деле в дальнейшем будем решать иммено такие задачи, на поиск оптимального параметра. Само решение задачи линейной бинарной классификации будет на следующей лекции. Сейчас воспользуемся библиотеками для решения данной задачи.\n",
    "\n",
    "### Поиск оптимального вектора параметров\n",
    "Перейдем к двум матрицам:\n",
    "1. Матрице объектов $\\mathbf{X} \\in \\mathbb{R}^{l\\times (n+1)}$\n",
    "2. Вектору ответов $\\mathbf{y} \\in \\{-1,1\\}^l$\n",
    "\n",
    "Заметим, что объекты мы преобразовали в пространство более большой размерности, добавив еще один признак, который у всех объектов будет равен $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-declaration",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = binary_dataset.loc[:, binary_dataset.columns != 'класс'].values\n",
    "y = binary_dataset.loc[:, 'класс'].values.reshape(-1)\n",
    "X = np.array(np.hstack([X, np.ones([len(X), 1])]), dtype=np.float64)\n",
    "y = np.array(y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=0, max_iter=2000)\n",
    "_ = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-globe",
   "metadata": {},
   "source": [
    "Получаем вектор оптимальных параметров $\\hat{\\theta}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-romantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed81d073",
   "metadata": {},
   "source": [
    "Используем модель для предсказания классов обьектов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X[[86, 2, 55]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-conditions",
   "metadata": {},
   "source": [
    "# Выбор модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng()\n",
    "vals = rng.standard_normal(10)\n",
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.arange(0, 10, 0.05)\n",
    "Y_real = 0.07*X*X + X + 2*np.cos(X) + 2\n",
    "Y_train = Y_real + 1.5 * rng.standard_normal(X.size)\n",
    "\n",
    "X_train1 = np.stack([X, np.cos(X)], axis=-1)\n",
    "reg1 = LinearRegression().fit(X_train1, Y_train)\n",
    "\n",
    "X_train2 = np.stack([X, X*X], axis=-1)\n",
    "reg2 = LinearRegression().fit(X_train2, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-unknown",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(8)\n",
    "ax.scatter(X, Y_train, s=25, label='Training data')\n",
    "ax.plot(X, reg1.predict(X_train1), 'g', label=r\"$Y=\\theta_2 cos(X) + \\theta_1 X + \\theta_0$\")\n",
    "ax.plot(X, reg2.predict(X_train2), 'r', label=r\"$Y=\\theta_2 X^2 + \\theta_1 X + \\theta_0$\")\n",
    "legend = ax.legend(loc='upper left', fontsize='x-large')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.savefig(\"regression_model.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-skirt",
   "metadata": {},
   "source": [
    "# Классификация и регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-disco",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Classification\n",
    "\n",
    "# we create 50 separable points\n",
    "X, Y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.90)\n",
    "\n",
    "# fit the model\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "xx = np.linspace(-2, 5, 10)\n",
    "yy = np.linspace(-1, 5, 10)\n",
    "\n",
    "X1, X2 = np.meshgrid(xx, yy)\n",
    "Z = np.empty(X1.shape)\n",
    "for (i, j), val in np.ndenumerate(X1):\n",
    "    x1 = val\n",
    "    x2 = X2[i, j]\n",
    "    p = clf.decision_function([[x1, x2]])\n",
    "    Z[i, j] = p[0]\n",
    "levels = [0.0]\n",
    "linestyles = [\"solid\"]\n",
    "colors = \"k\"\n",
    "\n",
    "print(Z)\n",
    "\n",
    "plt.figure(figsize=(12.8, 4.8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n",
    "plt.scatter(X[Y == 1, 0], X[Y == 1, 1], marker='x', s=25, color='k')\n",
    "plt.scatter(X[Y == 0, 0], X[Y == 0, 1], marker='o', s=25, facecolors='none', edgecolors='k')\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "\n",
    "# Regression\n",
    "\n",
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(100) + 1.5\n",
    "X.sort()\n",
    "res = 0.5 * np.random.randn(100)\n",
    "y = 2 + 0.05 * X * X + res\n",
    "#X_train = X.reshape(-1, 1)\n",
    "X_train = np.column_stack((X, np.square(X)))\n",
    "reg = make_pipeline(StandardScaler(), SGDRegressor())\n",
    "reg = reg.fit(X_train, y)\n",
    "ypred = reg.predict(X_train)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.plot(X, ypred, color='k')\n",
    "plt.scatter(X, y, s=25, facecolors='none', edgecolors='k')\n",
    "plt.savefig(\"cls_reg.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-logging",
   "metadata": {},
   "source": [
    "# Переобучение и контроль качества"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "concrete-contrary",
   "metadata": {},
   "source": [
    "В предыдущих примерах мы показали, что полученная оптимальная функция сильно зависит от данных и от сложности выбранной модели.\n",
    "\n",
    "Поэтому обычно выделяют этапы обучения и контроля. На этап обучения одни данные, а на этапе контроля другие.\n",
    "Перепишем постановку задачи следующем образом, пусть задана обучающая выборка:\n",
    "$$\\mathbf{X}_{tr} \\in \\mathbb{R}^{l_{tr}\\times n}, \\quad \\mathbf{y}_{tr} \\in \\mathbb{Y}^{l_{tr}},$$\n",
    "и выборка для контроля:\n",
    "$$\\mathbf{X}_{vl} \\in \\mathbb{R}^{l_{vl}\\times n}, \\quad \\mathbf{y}_{vl} \\in \\mathbb{Y}^{l_{vl}}.$$\n",
    "\n",
    "Также еще есть отложенная выборка, которая вообще не используется в подборе каких либо параметров, тестова выборка:\n",
    "$$\\mathbf{X}_{ts} \\in \\mathbb{R}^{l_{ts}\\times n}, \\quad \\mathbf{y}_{ts} \\in \\mathbb{Y}^{l_{ts}}.$$\n",
    "\n",
    "В этом случае оптимальные параметры $\\hat{\\theta}$ находятся из оптимизационной задачи:\n",
    "$$\\hat{\\theta} = \\arg \\min_{\\theta \\in \\mathbb{R}^n} Q\\bigr(\\theta, \\mathbf{X}_{tr}, \\mathbf{y}_{tr}\\bigr).$$\n",
    "\n",
    "После этапа получения параметров $\\hat{\\theta}$ происходит измерения качества модели на выборке $\\mathbf{X}_{vl}, \\mathbf{y}_{vl}$.\n",
    "\n",
    "И того получаем две ошибки $Q\\bigr(\\theta, \\mathbf{X}_{tr}, \\mathbf{y}_{tr}\\bigr)$ и $Q\\bigr(\\theta, \\mathbf{X}_{vl}, \\mathbf{y}_{vl}\\bigr)$ и в случае, если ошибка на обучении много меньше чем ошибка на контроле, то получаем переобучение."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-batman",
   "metadata": {},
   "source": [
    "## Борьба с переобучением\n",
    "Бороться с переобучениям можно многими способами:\n",
    "1. Изменения структуры модели\n",
    "2. Добавление регуляризаторов\n",
    "3. ...\n",
    "\n",
    "Но большинство методов борьбы с регуляризацией добавляют свои параметры (гиперпараметры), которые также нужно оптимизировать. Обычно для их оптимизации и используется метод Cross-Validation и Leave One Out.\n",
    "\n",
    "### LOO\n",
    "\n",
    "Один из простых методов борьбы с переобучением является метод Leave One Out. Для удобства обозначим $\\hat{\\theta}\\bigr(\\mathbf{X}\\bigr)$ как оптимальный вектор для выборки $\\mathbf{X}$. Тогда ошибка LOO определяется следующим образом:\n",
    "$$LOO\\bigr(\\mu, \\textbf{X}_{tr}, \\textbf{y}_{tr}, \\textbf{X}_{vl}, \\textbf{y}_{vl}\\bigr) = \n",
    "\\sum_{i=1}^{l_{tr}+l_{vl}}q\\bigr(\\hat{\\theta}\\bigr(\\mathbf{X}_{tr}\\cup\\textbf{X}_{vl}\\setminus\\mathbf{x}_i, \\mu\\bigr), \\mathbf{x}_i, y_i\\bigr),$$\n",
    "где $\\mathbf{x}_i$ это элемент из объединенного датасета обучения и валидации.\n",
    "\n",
    "После чего оптимальный вектор параметров является решением следующей оптимизационной задачи:\n",
    "$$\n",
    "\\hat{\\mu} = \\arg\\min_{\\mu \\in \\mathfrak{M}} LOO\\bigr(\\mu, \\textbf{X}_{tr}, \\textbf{y}_{tr}, \\textbf{X}_{vl}, \\textbf{y}_{vl}\\bigr),\n",
    "$$\n",
    "$$\n",
    "\\hat{\\theta} = \\hat{\\theta}\\bigr(\\mathbf{X}_{tr}\\cup\\textbf{X}_{vl}, \\hat{\\mu}\\bigr)\n",
    "$$\n",
    "\n",
    "Для нашего синтетического примера в качестве параметра $\\mu$ можно рассмоттреть степень полинома для аппроксимации (изменения структуры модели).\n",
    "\n",
    "Пример, где реально используется LOO или CV будет рассмотрен на одном из следующих семинаров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a96ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline(\n",
    "        [\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression),\n",
    "        ]\n",
    "    )\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(\n",
    "        pipeline, X[:, np.newaxis], y, scoring=\"neg_mean_squared_error\", cv=10\n",
    "    )\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor=\"b\", s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\n",
    "        \"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "            degrees[i], -scores.mean(), scores.std()\n",
    "        )\n",
    "    )\n",
    "plt.savefig(\"overfitting.svg\", format=\"svg\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-divorce",
   "metadata": {},
   "source": [
    "# Разные задачи машинного обучения\n",
    "\n",
    "Задачи машиного обучения можно разделить на несколько типов:\n",
    "1. Решается одна прикладная задача. В данном случае основной целью является получить наилучший результат на отложенной выборке. В данном случае обычно есть одна выборка и нужно перебрать все возможные методы машинного обучения, чтобы получить лучший результат.\n",
    "2. Тестируется метод на большом количестве реальных данных. В данном случае основной целью является протестировать новый метод на широком спектре задач. Рассматриваются задачи из разных областей.\n",
    "3. Тестирования метода на большом количестве синтетических данных/ полусинтеттиических данных. Синтетические данные позволяют проверять разные гипотезы о модели. \n",
    "\n",
    "## Прикладные задачи\n",
    "\n",
    "Обычно решение таких задач состоит из следующих этапов:\n",
    "1. Ищуться модели, которые решают похожие задачи.\n",
    "2. Анализ данных, анализ области откуда пришла задача.\n",
    "3. Адаптация моделей под прикладную задачу.\n",
    "4. Поверхностная оценка качества каждой модели на полученной выборке.\n",
    "5. Обучения 1 или 2 лучших моделей, подбирая все возможные параметры и гиперпараметры для получения лучшего результата.\n",
    "\n",
    "Главный результат данной задачи, это получить модель (одну функцию из модели алгоритмов), которая решает конкретную задачу, на конкретном наборе данных.\n",
    "\n",
    "## Исследовательские задачи\n",
    "\n",
    "Обычно решение таких задач состоит из следующих этапов:\n",
    "1. Анализ некоторой предметной области\n",
    "2. Разработка новой модели машиного обучения\n",
    "3. Проверка работоспособности метода на синтетических данных\n",
    "4. Выявление ограничений метода\n",
    "5. Тестирование на большом количестве реальных данных \n",
    "\n",
    "Заметим, что основным результатом в данном случае является не одна функция, которая решает одну поставленую задачу, а новая модель алгоритмов, которая может быть применима для большого количества задач. Важно, что не обязательно, новая модель алгоритмов должна давать лучшее качество на разных выборках."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-criticism",
   "metadata": {},
   "source": [
    "## Домашнее задание\n",
    "\n",
    "- Пройти курс: [Coursera Machine Learning MOOC by Andrew Ng](https://www.coursera.org/learn/machine-learning)\n",
    "- Задания к курсу на python можно найти здесь: [ml-coursera-python-assignments](https://github.com/dibgerge/ml-coursera-python-assignments)\n",
    "- К следующему семинару выполнить задание 1 и быть готовым презентовать"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 08:28:41) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "3e00b96764db5b0cdf5cee97e59d5a848d5550a9221e175d3badd9d15aef4d1f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
